{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rohit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Rohit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+', '', text)  # Remove URLs, usernames, and mentions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join the tokens back into a single string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r'R:\\Big Data Analytics  Lambton\\Sem 3\\AML 2034 - Bhavik Gandhi\\Project\\bert_model\\final_ats_scores9.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "df['cleaned_resume'] = df['Resume_str'].apply(preprocess_text)\n",
    "df['cleaned_description'] = df['description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features and labels\n",
    "train_resumes = train_df['cleaned_resume'].tolist()\n",
    "train_descriptions = train_df['cleaned_description'].tolist()\n",
    "train_labels = train_df['ATS_Score'].tolist()\n",
    "\n",
    "val_resumes = val_df['cleaned_resume'].tolist()\n",
    "val_descriptions = val_df['cleaned_description'].tolist()\n",
    "val_labels = val_df['ATS_Score'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load DistilBERT models and tokenizers\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class ATSScorePredictionModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ATSScorePredictionModel, self).__init__()\n",
    "        self.distilbert_resume = distilbert_model\n",
    "        self.distilbert_jd = distilbert_model\n",
    "        self.dense_resume = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense_jd = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.concat_layer = tf.keras.layers.Concatenate()\n",
    "        self.final_dense = tf.keras.layers.Dense(1, activation='linear')  # Regression output\n",
    "\n",
    "    def call(self, inputs):\n",
    "        resume_input, jd_input = inputs\n",
    "\n",
    "        # Ensure inputs have correct shape\n",
    "        resume_input = resume_input[:, 0, :]  # Remove unnecessary dimensions\n",
    "        jd_input = jd_input[:, 0, :]  # Remove unnecessary dimensions\n",
    "        \n",
    "        # Process resume\n",
    "        resume_embeddings = self.distilbert_resume(resume_input)[0]\n",
    "        resume_pooled = tf.reduce_mean(resume_embeddings, axis=1)\n",
    "        resume_output = self.dense_resume(resume_pooled)\n",
    "        \n",
    "        # Process JD\n",
    "        jd_embeddings = self.distilbert_jd(jd_input)[0]\n",
    "        jd_pooled = tf.reduce_mean(jd_embeddings, axis=1)\n",
    "        jd_output = self.dense_jd(jd_pooled)\n",
    "        \n",
    "        # Concatenate and final prediction\n",
    "        combined_output = self.concat_layer([resume_output, jd_output])\n",
    "        final_output = self.final_dense(combined_output)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "model = ATSScorePredictionModel()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inputs(resume_text, jd_text):\n",
    "    resume_tokens = tokenizer(resume_text, return_tensors='tf', max_length=512, truncation=True, padding='max_length')\n",
    "    jd_tokens = tokenizer(jd_text, return_tensors='tf', max_length=512, truncation=True, padding='max_length')\n",
    "    return resume_tokens, jd_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(resumes, descriptions, labels, batch_size=8):\n",
    "    def gen():\n",
    "        for resume, description, label in zip(resumes, descriptions, labels):\n",
    "            resume_tokens = tokenizer(resume, return_tensors='tf', max_length=512, truncation=True, padding='max_length')\n",
    "            description_tokens = tokenizer(description, return_tensors='tf', max_length=512, truncation=True, padding='max_length')\n",
    "            yield (resume_tokens['input_ids'], description_tokens['input_ids']), label\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(None, 512), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(None, 512), dtype=tf.int32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = create_dataset(train_resumes, train_descriptions, train_labels)\n",
    "val_dataset = create_dataset(val_resumes, val_descriptions, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=2  # Adjust the number of epochs as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From c:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "744/744 [==============================] - 7047s 9s/step - loss: 630.9871 - mae: 21.2958 - val_loss: 293.1325 - val_mae: 14.6339\n",
      "Epoch 2/2\n",
      "744/744 [==============================] - 7138s 10s/step - loss: 230.0740 - mae: 12.3736 - val_loss: 190.3598 - val_mae: 10.9442\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=2  # Adjust the number of epochs as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "744/744 [==============================] - 7298s 10s/step - loss: 168.4656 - mae: 10.2078 - val_loss: 142.6405 - val_mae: 9.4702\n",
      "Epoch 2/3\n",
      "744/744 [==============================] - 7483s 10s/step - loss: 146.6543 - mae: 9.2664 - val_loss: 137.5253 - val_mae: 8.9944\n",
      "Epoch 3/3\n",
      "744/744 [==============================] - 10708s 14s/step - loss: 134.9695 - mae: 8.8683 - val_loss: 127.5471 - val_mae: 8.8692\n"
     ]
    }
   ],
   "source": [
    "# Continue training for more epochs\n",
    "history_continued = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('ats_score_prediction_model1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# # Streamlit UI to show explanations (if needed)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# if st.button(\"Explain Prediction\"):\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_input \u001b[38;5;129;01mand\u001b[39;00m jd_input:\n\u001b[1;32m---> 36\u001b[0m     explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplain_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjd_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplanation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, explanation)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m, in \u001b[0;36mexplain_prediction\u001b[1;34m(resume_text, jd_text)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert tokens to the format expected by LIME\u001b[39;00m\n\u001b[0;32m     13\u001b[0m combined_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([resume_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(), jd_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explanation\n",
      "File \u001b[1;32mc:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lime\\lime_text.py:409\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    369\u001b[0m                      text_instance,\n\u001b[0;32m    370\u001b[0m                      classifier_fn,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m                      distance_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    376\u001b[0m                      model_regressor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    377\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates explanations for a prediction.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m    First, we generate neighborhood data by randomly hiding features from\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        explanations.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     indexed_string \u001b[38;5;241m=\u001b[39m (IndexedCharacters(\n\u001b[0;32m    407\u001b[0m         text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow, mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string)\n\u001b[0;32m    408\u001b[0m                       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m                       \u001b[43mIndexedString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msplit_expression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmask_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_string\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    412\u001b[0m     domain_mapper \u001b[38;5;241m=\u001b[39m TextDomainMapper(indexed_string)\n\u001b[0;32m    413\u001b[0m     data, yss, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_labels_distances(\n\u001b[0;32m    414\u001b[0m         indexed_string, classifier_fn, num_samples,\n\u001b[0;32m    415\u001b[0m         distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lime\\lime_text.py:114\u001b[0m, in \u001b[0;36mIndexedString.__init__\u001b[1;34m(self, raw_string, split_expression, bow, mask_string)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# with the split_expression as a non-capturing group (?:), we don't need to filter out\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# the separator character from the split results.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     splitter \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)|$\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m split_expression)\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_list \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m s]\n\u001b[0;32m    115\u001b[0m     non_word \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39mmatch\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_list)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# Define a LIME explainer\n",
    "explainer = LimeTextExplainer()\n",
    "\n",
    "def explain_prediction(resume_text, jd_text):\n",
    "    resume_input = preprocess_text(resume_text)\n",
    "    jd_input = preprocess_text(jd_text)\n",
    "    resume_tokens, jd_tokens = tokenize_inputs(resume_input, jd_input)\n",
    "    \n",
    "    # Convert tokens to the format expected by LIME\n",
    "    combined_input = np.concatenate([resume_tokens['input_ids'].numpy(), jd_tokens['input_ids'].numpy()], axis=1)\n",
    "    \n",
    "    explanation = explainer.explain_instance(combined_input, model.predict)\n",
    "    return explanation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if resume_input and jd_input:\n",
    "    explanation = explain_prediction(resume_input, jd_input)\n",
    "    print(\"Explanation:\", explanation)\n",
    "else:\n",
    "    print(\"Please provide both resume and job description texts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_regression_prediction(resume, job_description, model, tokenizer):\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    import numpy as np\n",
    "    \n",
    "    # Preprocess the inputs using the existing preprocess_text function\n",
    "    preprocessed_resume = preprocess_text(resume)\n",
    "    preprocessed_description = preprocess_text(job_description)\n",
    "    \n",
    "    # Combine the preprocessed texts\n",
    "    combined_text = preprocessed_resume + \" \" + preprocessed_description\n",
    "    \n",
    "    # Initialize LIME Text Explainer (no mode parameter)\n",
    "    explainer = LimeTextExplainer()\n",
    "    \n",
    "    # Define the prediction function for LIME\n",
    "    def predict_fn(texts):\n",
    "        tokenized_inputs = []\n",
    "        for text in texts:\n",
    "            tokens = tokenize_inputs(text, tokenizer)\n",
    "            tokenized_inputs.append(tokens)\n",
    "        \n",
    "        # Stack the inputs into a single tensor for prediction\n",
    "        tokenized_inputs = np.vstack(tokenized_inputs)\n",
    "        predictions = model.predict(tokenized_inputs)\n",
    "        return predictions.flatten()  # Flatten to return a 1D array of continuous predictions\n",
    "    \n",
    "    # Generate the explanation using LIME\n",
    "    exp = explainer.explain_instance(combined_text, predict_fn)\n",
    "    return exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sample_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate LIME explanation for regression\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplain_regression_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_resume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Display the explanation\u001b[39;00m\n\u001b[0;32m      9\u001b[0m explanation\u001b[38;5;241m.\u001b[39mshow_in_notebook()\n",
      "Cell \u001b[1;32mIn[26], line 28\u001b[0m, in \u001b[0;36mexplain_regression_prediction\u001b[1;34m(resume, job_description, model, tokenizer)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Flatten to return a 1D array of continuous predictions\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Generate the explanation using LIME\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp\n",
      "File \u001b[1;32mc:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lime\\lime_text.py:413\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    406\u001b[0m indexed_string \u001b[38;5;241m=\u001b[39m (IndexedCharacters(\n\u001b[0;32m    407\u001b[0m     text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow, mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string)\n\u001b[0;32m    408\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m    409\u001b[0m                   IndexedString(text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow,\n\u001b[0;32m    410\u001b[0m                                 split_expression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_expression,\n\u001b[0;32m    411\u001b[0m                                 mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string))\n\u001b[0;32m    412\u001b[0m domain_mapper \u001b[38;5;241m=\u001b[39m TextDomainMapper(indexed_string)\n\u001b[1;32m--> 413\u001b[0m data, yss, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_labels_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexed_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(yss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "File \u001b[1;32mc:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lime\\lime_text.py:482\u001b[0m, in \u001b[0;36mLimeTextExplainer.__data_labels_distances\u001b[1;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[0;32m    480\u001b[0m     data[i, inactive] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    481\u001b[0m     inverse_data\u001b[38;5;241m.\u001b[39mappend(indexed_string\u001b[38;5;241m.\u001b[39minverse_removing(inactive))\n\u001b[1;32m--> 482\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m distances \u001b[38;5;241m=\u001b[39m distance_fn(sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix(data))\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, labels, distances\n",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m, in \u001b[0;36mexplain_regression_prediction.<locals>.predict_fn\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m---> 19\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     tokenized_inputs\u001b[38;5;241m.\u001b[39mappend(tokens)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Stack the inputs into a single tensor for prediction\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36mtokenize_inputs\u001b[1;34m(resume_text, jd_text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_inputs\u001b[39m(resume_text, jd_text):\n\u001b[0;32m      2\u001b[0m     resume_tokens \u001b[38;5;241m=\u001b[39m tokenizer(resume_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     jd_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjd_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resume_tokens, jd_tokens\n",
      "File \u001b[1;32mc:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2945\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2944\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2945\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Rohit Kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3004\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3001\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 3004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3005\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3007\u001b[0m     )\n\u001b[0;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   3010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3013\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Select a sample resume and job description from the dataset\n",
    "sample_resume = str(df['Resume_str'].iloc[0])\n",
    "sample_description = str(df['description'].iloc[0])\n",
    "\n",
    "# Generate LIME explanation for regression\n",
    "explanation = explain_regression_prediction(sample_resume, sample_description, model, tokenizer)\n",
    "\n",
    "# Display the explanation\n",
    "explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
